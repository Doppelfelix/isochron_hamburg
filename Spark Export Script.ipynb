{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ecfd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/20 14:01:51 WARN Utils: Your hostname, felix-ubuntu20 resolves to a loopback address: 127.0.1.1; using 192.168.178.25 instead (on interface wlo1)\n",
      "22/08/20 14:01:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/20 14:01:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/20 14:01:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/20 14:01:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/08/20 14:01:52 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/08/20 14:01:54 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "from pyspark.sql.window import Window\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "PATH_TO_S3_JSON_FOLDER = \"s3a://biking-data/*\"\n",
    "PATH_TO_S3_RESULTS_FOLDER = \"s3a://biking-results/\"\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Aggregating Hamburg Bike Stations\")\n",
    "    .config(\"spark.pyspark.python\", \"python\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "df = spark.read.format(\"json\").load(PATH_TO_S3_JSON_FOLDER, multiLine=True)\n",
    "\n",
    "exploded_df = df.select(\n",
    "    df.thingID,\n",
    "    df.description,\n",
    "    df.coordinatesX,\n",
    "    df.coordinatesY,\n",
    "    F.explode(df.obs).alias(\"obs\"),\n",
    ")\n",
    "\n",
    "obs_df = exploded_df.select(\n",
    "    exploded_df.thingID,\n",
    "    exploded_df.description,\n",
    "    exploded_df.coordinatesX,\n",
    "    exploded_df.coordinatesY,\n",
    "    exploded_df.obs.observationID.alias(\"observationID\"),\n",
    "    exploded_df.obs.Result.alias(\"Result\"),\n",
    "    exploded_df.obs.resultTime.alias(\"resultTime\"),\n",
    ")\n",
    "\n",
    "obs_df = obs_df.withColumn(\n",
    "    \"resultTime\", F.to_timestamp(\"resultTime\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ")\n",
    "obs_df = obs_df.withColumn(\"resultHour\", F.hour(obs_df.resultTime))\n",
    "obs_df = obs_df.withColumn(\"resultWeekday\", F.dayofweek(obs_df.resultTime))\n",
    "obs_df = obs_df.withColumn(\"resultTimeTrunct\", F.date_trunc(\"hour\", obs_df.resultTime))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9d0b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8700212"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b812343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = obs_df.groupby(\n",
    "    \"thingID\", \"description\", \"coordinatesX\", \"coordinatesY\", \"resultTimeTrunct\"\n",
    ").agg(F.mean(\"Result\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc73d904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "min_date_hour = datetime.datetime(2019, 7, 26, 9, 0)\n",
    "max_date_hour = agg_df.select(F.max('resultTimeTrunct')).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c303f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta as td, datetime\n",
    "def get_delta(d1, d2):\n",
    "    delta = d2 - d1\n",
    "    return delta\n",
    "\n",
    "delta = get_delta(min_date_hour,max_date_hour)\n",
    "hour_list = []\n",
    "for i in range(delta.days * 24 + 1):\n",
    "    hour_list.append(min_date_hour + td(hours=i))\n",
    "all_hours_df = spark.createDataFrame([{\"resultTimeTrunct\": x} for x in hour_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d62d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = (\n",
    "    all_hours_df.crossJoin(agg_df.select(\"thingID\").distinct())\n",
    "    .join(agg_df, [\"thingID\", \"resultTimeTrunct\"], \"left\")\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ff30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec  = Window.partitionBy(\"thingID\").orderBy(\"resultTimeTrunct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7bdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.withColumn('Result', F.coalesce('avg(Result)', F.last('avg(Result)', True).over(windowSpec)))\n",
    "result_df = result_df.withColumn('description', F.coalesce('description', F.last('description', True).over(windowSpec)))\n",
    "result_df = result_df.withColumn('coordinatesX', F.coalesce('coordinatesX', F.last('coordinatesX', True).over(windowSpec)))\n",
    "result_df = result_df.withColumn('coordinatesY', F.coalesce('coordinatesY', F.last('coordinatesY', True).over(windowSpec)))\n",
    "result_df = result_df.drop('avg(Result)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2403a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = result_df.withColumn(\"resultHour\", F.hour(result_df.resultTimeTrunct))\n",
    "final_df = final_df.withColumn(\"resultWeekday\", F.dayofweek(final_df.resultTimeTrunct))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b816e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_agg_df = final_df.groupby(\n",
    "    \"thingID\", \"description\", \"coordinatesX\", \"coordinatesY\", \"resultHour\", \"resultWeekday\"\n",
    ").agg(F.mean(\"Result\").alias('average_res'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bedb914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_df = final_agg_df.filter(final_agg_df.average_res.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22965c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "non_null_df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(\n",
    "    \"results/all_stations_by_weekday_hour.csv\", mode=\"overwrite\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0079251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
